#!/usr/bin/env python
"""
Inference Script for ViT Classification Model
å°å–®å¼µæˆ–å¤šå¼µåœ–ç‰‡é€²è¡Œæ¨ç†é æ¸¬

Usage:
    # å–®å¼µåœ–ç‰‡æ¨ç†
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/image.jpg
    
    # è³‡æ–™å¤¾æ‰¹æ¬¡æ¨ç†
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/images/
    
    # é¡¯ç¤º Top-5 é æ¸¬
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/image.jpg --top-k 5
"""

import argparse
import os
import glob
import yaml
from typing import List

from PIL import Image
import numpy as np
import torch

from models.classifier import ViTMAEForImageClassification


def parse_args():
    parser = argparse.ArgumentParser(description="Inference for trained ViT classifier")
    parser.add_argument("--model-dir", required=True, help="Directory with saved model")
    parser.add_argument("--input", required=True, help="Image file or directory for inference")
    parser.add_argument("--config", required=True, help="Training YAML config file")
    parser.add_argument("--top-k", type=int, default=1, help="Show top-k predictions (default: 1)")
    parser.add_argument("--device", default=None, help="cuda or cpu (default: auto)")
    parser.add_argument("--show-probs", action="store_true", help="Show prediction probabilities")
    return parser.parse_args()


def preprocess_image(path: str, img_h: int, img_w: int) -> torch.Tensor:
    """é è™•ç†åœ–ç‰‡ï¼Œèˆ‡è¨“ç·´æ™‚ä¸€è‡´"""
    img = Image.open(path).convert("RGB")
    w, h = img.size
    
    # ä¿æŒé•·å¯¬æ¯”ç¸®æ”¾
    scale = min(img_w / w, img_h / h)
    nw, nh = int(w * scale), int(h * scale)
    img = img.resize((nw, nh), Image.BILINEAR)
    
    # ç½®ä¸­å¡«å……é»‘é‚Š
    canvas = Image.new("RGB", (img_w, img_h), (0, 0, 0))
    left = (img_w - nw) // 2
    top = (img_h - nh) // 2
    canvas.paste(img, (left, top))
    
    # è½‰æ›ç‚º tensor ä¸¦æ­£è¦åŒ–
    arr = np.array(canvas).astype("float32") / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype="float32")
    std = np.array([0.229, 0.224, 0.225], dtype="float32")
    arr = (arr - mean) / std
    arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW
    
    return torch.from_numpy(arr)


def load_lexicon(root_dir: str) -> List[str]:
    """è¼‰å…¥è©å½™è¡¨"""
    lexicon_path = os.path.join(root_dir, "lexicon.txt")
    if os.path.exists(lexicon_path):
        with open(lexicon_path, "r") as f:
            return [line.strip() for line in f]
    return []


def main():
    args = parse_args()
    
    # è¨­å®šè£ç½®
    if args.device:
        device = torch.device(args.device)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    # è¼‰å…¥é…ç½®
    with open(args.config, "r") as f:
        cfg = yaml.safe_load(f)
    
    data_cfg = cfg.get("data", {})
    img_h = data_cfg.get("img_height", 48)
    img_w = data_cfg.get("img_width", 96)
    
    print(f"ğŸ“ Image size: {img_h} Ã— {img_w}")
    
    # è¼‰å…¥è©å½™è¡¨
    lexicon = load_lexicon(data_cfg.get("root", "dataset/minDataset"))
    if not lexicon:
        print("âš ï¸  Warning: Could not load lexicon.txt, will show label indices instead")
    else:
        print(f"ğŸ“– Loaded lexicon with {len(lexicon)} words")
    
    # è¼‰å…¥æ¨¡å‹
    print(f"ğŸ“‚ Loading model from: {args.model_dir}")
    model = ViTMAEForImageClassification.from_pretrained(args.model_dir)
    model.to(device)
    model.eval()
    
    # è¨ˆç®—åƒæ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    
    # æ”¶é›†åœ–ç‰‡
    if os.path.isdir(args.input):
        exts = ("png", "jpg", "jpeg", "bmp", "PNG", "JPG", "JPEG")
        imgs = []
        for ext in exts:
            imgs.extend(glob.glob(os.path.join(args.input, f"**/*.{ext}"), recursive=True))
        imgs = sorted(imgs)
    else:
        imgs = [args.input]
    
    if len(imgs) == 0:
        raise RuntimeError("âŒ No input images found for inference")
    
    print(f"\nğŸ–¼ï¸  Found {len(imgs)} image(s) to process\n")
    print("=" * 70)
    
    # æ‰¹æ¬¡è™•ç†
    batch_tensors = [preprocess_image(p, img_h, img_w) for p in imgs]
    batch_tensor = torch.stack(batch_tensors).to(device)
    
    with torch.no_grad():
        outputs = model(pixel_values=batch_tensor, return_dict=True)
        logits = outputs.logits  # (B, num_classes)
        probs = torch.softmax(logits, dim=-1)
    
    # è¼¸å‡ºé æ¸¬çµæœ
    for i, (path, prob) in enumerate(zip(imgs, probs)):
        filename = os.path.basename(path)
        
        # å–å¾— top-k é æ¸¬
        top_probs, top_indices = prob.topk(args.top_k)
        
        print(f"ğŸ“„ {filename}")
        
        for rank, (idx, p) in enumerate(zip(top_indices.cpu().tolist(), top_probs.cpu().tolist()), 1):
            if lexicon and idx < len(lexicon):
                word = lexicon[idx]
            else:
                word = f"LABEL_{idx}"
            
            if args.show_probs or args.top_k > 1:
                print(f"   #{rank}: {word} ({p*100:.2f}%)")
            else:
                print(f"   Prediction: {word}")
        
        if i < len(imgs) - 1:
            print("-" * 70)
    
    print("=" * 70)
    print(f"\nâœ… Inference completed for {len(imgs)} image(s)")


if __name__ == "__main__":
    main()
