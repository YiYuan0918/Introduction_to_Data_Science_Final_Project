#!/usr/bin/env python
"""
Inference Script for ViT Classification Model
å°å–®å¼µæˆ–å¤šå¼µåœ–ç‰‡é€²è¡Œæ¨ç†é æ¸¬

Usage:
    # å–®å¼µåœ–ç‰‡æ¨ç†
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/image.jpg
    
    # è³‡æ–™å¤¾æ‰¹æ¬¡æ¨ç†
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/images/
    
    # é¡¯ç¤º Top-5 é æ¸¬
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/image.jpg --top-k 5
    
    # é©—è­‰é æ¸¬æ˜¯å¦æ­£ç¢ºï¼ˆè‡ªå‹•æœå°‹ train/val/test æ¨™è¨»ï¼‰
    python infer.py --model-dir outputs/classifier --config configs/cls.yaml --input path/to/image.jpg \
        --top-k 5 --show-probs --verify
"""

import argparse
import os
import glob
import yaml
from typing import List

from PIL import Image
import numpy as np
import torch

from models.classifier import ViTMAEForImageClassification


def parse_args():
    parser = argparse.ArgumentParser(description="Inference for trained ViT classifier")
    parser.add_argument("--model-dir", required=True, help="Directory with saved model")
    parser.add_argument("--input", required=True, help="Image file or directory for inference")
    parser.add_argument("--config", required=True, help="Training YAML config file")
    parser.add_argument("--top-k", type=int, default=1, help="Show top-k predictions (default: 1)")
    parser.add_argument("--device", default=None, help="cuda or cpu (default: auto)")
    parser.add_argument("--show-probs", action="store_true", help="Show prediction probabilities")
    parser.add_argument("--verify", action="store_true",
                        help="Compare predictions against ground-truth labels (auto search train/val/test)")
    return parser.parse_args()


def preprocess_image(path: str, img_h: int, img_w: int) -> torch.Tensor:
    """é è™•ç†åœ–ç‰‡ï¼Œèˆ‡è¨“ç·´æ™‚ä¸€è‡´"""
    img = Image.open(path).convert("RGB")
    w, h = img.size
    
    # ä¿æŒé•·å¯¬æ¯”ç¸®æ”¾
    scale = min(img_w / w, img_h / h)
    nw, nh = int(w * scale), int(h * scale)
    img = img.resize((nw, nh), Image.BILINEAR)
    
    # ç½®ä¸­å¡«å……é»‘é‚Š
    canvas = Image.new("RGB", (img_w, img_h), (0, 0, 0))
    left = (img_w - nw) // 2
    top = (img_h - nh) // 2
    canvas.paste(img, (left, top))
    
    # è½‰æ›ç‚º tensor ä¸¦æ­£è¦åŒ–
    arr = np.array(canvas).astype("float32") / 255.0
    mean = np.array([0.485, 0.456, 0.406], dtype="float32")
    std = np.array([0.229, 0.224, 0.225], dtype="float32")
    arr = (arr - mean) / std
    arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW
    
    return torch.from_numpy(arr)


def load_lexicon(root_dir: str) -> List[str]:
    """è¼‰å…¥è©å½™è¡¨"""
    lexicon_path = os.path.join(root_dir, "lexicon.txt")
    if os.path.exists(lexicon_path):
        with open(lexicon_path, "r") as f:
            return [line.strip() for line in f]
    return []


def load_annotations(root_dir: str, split: str):
    """è¼‰å…¥æŒ‡å®š split çš„æ¨™è¨»ï¼Œå›å‚³ ({rel_path: label_id}, {basename: (label_id, rel_path)})"""
    ann_path = os.path.join(root_dir, f"annotation_{split}.txt")
    if not os.path.exists(ann_path):
        raise FileNotFoundError(f"Annotation file not found: {ann_path}")

    mapping = {}
    basename_map = {}
    with open(ann_path, "r", encoding="utf-8") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) != 2:
                continue
            rel_path, label = parts
            mapping[rel_path] = int(label)
            base = os.path.basename(rel_path)
            # è‹¥åŒåæª”æ¡ˆé‡è¤‡ï¼Œä¿ç•™ç¬¬ä¸€å€‹åŒ¹é…å³å¯
            if base not in basename_map:
                basename_map[base] = (int(label), rel_path)
    return mapping, basename_map


def main():
    args = parse_args()

    # è¨­å®šè£ç½®
    if args.device:
        device = torch.device(args.device)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ–¥ï¸  Using device: {device}")
    
    # è¼‰å…¥é…ç½®
    with open(args.config, "r") as f:
        cfg = yaml.safe_load(f)
    
    data_cfg = cfg.get("data", {})
    img_h = data_cfg.get("img_height", 48)
    img_w = data_cfg.get("img_width", 96)
    
    print(f"ğŸ“ Image size: {img_h} Ã— {img_w}")
    
    # è¼‰å…¥è©å½™è¡¨
    lexicon = load_lexicon(data_cfg.get("root", "dataset/minDataset"))
    if not lexicon:
        print("âš ï¸  Warning: Could not load lexicon.txt, will show label indices instead")
    else:
        print(f"ğŸ“– Loaded lexicon with {len(lexicon)} words")

    # è³‡æ–™æ ¹ç›®éŒ„èˆ‡æ¨™è¨»å¿«å–
    data_root = data_cfg.get("root", "dataset/minDataset")
    ann_cache = {}  # å¿«å–å·²è¼‰å…¥çš„æ¨™è¨» {split: (ann_map, basename_map)}
    
    # è¼‰å…¥æ¨¡å‹
    print(f"ğŸ“‚ Loading model from: {args.model_dir}")
    model = ViTMAEForImageClassification.from_pretrained(args.model_dir)
    model.to(device)
    model.eval()
    
    # è¨ˆç®—åƒæ•¸é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    
    # æ”¶é›†åœ–ç‰‡
    if os.path.isdir(args.input):
        exts = ("png", "jpg", "jpeg", "bmp", "PNG", "JPG", "JPEG")
        imgs = []
        for ext in exts:
            imgs.extend(glob.glob(os.path.join(args.input, f"**/*.{ext}"), recursive=True))
        imgs = sorted(imgs)
    else:
        imgs = [args.input]
    
    if len(imgs) == 0:
        raise RuntimeError("âŒ No input images found for inference")
    
    print(f"\nğŸ–¼ï¸  Found {len(imgs)} image(s) to process\n")
    print("=" * 70)
    
    # æ‰¹æ¬¡è™•ç†
    batch_tensors = [preprocess_image(p, img_h, img_w) for p in imgs]
    batch_tensor = torch.stack(batch_tensors).to(device)
    
    with torch.no_grad():
        outputs = model(pixel_values=batch_tensor, return_dict=True)
        logits = outputs.logits  # (B, num_classes)
        probs = torch.softmax(logits, dim=-1)
    
    # è¼¸å‡ºé æ¸¬çµæœ
    correct = 0
    found_gt = 0

    for i, (path, prob) in enumerate(zip(imgs, probs)):
        filename = os.path.basename(path)
        
        # å–å¾— top-k é æ¸¬
        top_probs, top_indices = prob.topk(args.top_k)
        
        print(f"ğŸ“„ {filename}")
        
        top_indices_list = top_indices.cpu().tolist()
        top_probs_list = top_probs.cpu().tolist()

        # Ground truth (if available)
        gt_label = None
        gt_word = None
        rel_path = os.path.relpath(path, data_root)
        rel_path_norm = rel_path.replace(os.sep, "/")
        basename = os.path.basename(path)

        def load_split_cache(split_name):
            if split_name not in ann_cache:
                ann_cache[split_name] = load_annotations(data_root, split_name)
            return ann_cache[split_name]

        # å°‹æ‰¾æ¨™è¨»ï¼šè‡ªå‹•åœ¨ train/val/test ä¸­æœå°‹
        found_split = None
        if args.verify:
            for sp in ["train", "val", "test"]:
                if sp in ann_cache:
                    amap, bmap = ann_cache[sp]
                else:
                    try:
                        amap, bmap = load_split_cache(sp)
                    except FileNotFoundError:
                        continue

                # ç²¾ç¢ºè·¯å¾‘åŒ¹é…
                if rel_path_norm in amap:
                    gt_label = amap[rel_path_norm]
                    found_split = sp
                # æª”ååŒ¹é…ï¼ˆè‹¥ç²¾ç¢ºæœªå‘½ä¸­ï¼‰
                elif basename in bmap:
                    gt_label, matched_rel = bmap[basename]
                    found_split = sp
                    # è¦†å¯«é¡¯ç¤ºç”¨çš„ç›¸å°è·¯å¾‘
                    rel_path_norm = matched_rel

                if gt_label is not None:
                    found_gt += 1
                    if lexicon and gt_label < len(lexicon):
                        gt_word = lexicon[gt_label]
                    else:
                        gt_word = f"LABEL_{gt_label}"
                    break

        for rank, (idx, p) in enumerate(zip(top_indices_list, top_probs_list), 1):
            if lexicon and idx < len(lexicon):
                word = lexicon[idx]
            else:
                word = f"LABEL_{idx}"
            
            if args.show_probs or args.top_k > 1:
                print(f"   #{rank}: {word} ({p*100:.2f}%)")
            else:
                print(f"   Prediction: {word}")

        # é©—è­‰æ­£ç¢ºæ€§ (Top-1)
        if gt_label is not None:
            top1_pred = top_indices_list[0]
            is_correct = top1_pred == gt_label
            if is_correct:
                correct += 1
            status = "âœ… CORRECT" if is_correct else "âŒ WRONG"
            print(f"   Ground Truth: {gt_word} (ID: {gt_label}) -> {status}")
        elif args.verify:
            print("   âš ï¸ Ground truth not found in annotations (path mismatch)")
        
        if i < len(imgs) - 1:
            print("-" * 70)
    
    print("=" * 70)
    if args.verify and found_gt > 0:
        acc = correct / found_gt * 100
        print(f"\nğŸ“Š Verification: {correct}/{found_gt} correct ({acc:.1f}%) [matched annotations]")
    print(f"\nâœ… Inference completed for {len(imgs)} image(s)")


if __name__ == "__main__":
    main()
